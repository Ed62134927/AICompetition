# PowerPulse é€²éš NLP ç³»çµ±
# åŒ…å«ï¼šABSAã€è©åµŒå…¥ã€èªç¾©èšé¡ã€è¦–è¦ºåŒ–ã€é›¶æ¨£æœ¬å­¸ç¿’

import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from transformers import pipeline
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import umap
import plotly.express as px
import plotly.graph_objects as go
from typing import List, Dict, Tuple
import jieba

# =======================================
# 1. æ–¹é¢ç´šæƒ…æ„Ÿåˆ†æ (ABSA)
# =======================================

class AspectBasedSentimentAnalyzer:
    """
    é€²éšæ–¹é¢ç´šæƒ…æ„Ÿåˆ†æç³»çµ±
    ä½¿ç”¨ä¸­æ–‡ RoBERTa é€²è¡Œç´°ç²’åº¦çš„ç”¢å“ç‰¹å¾µæƒ…æ„Ÿåˆ†æ
    """
    
    def __init__(self, model_name='ckiplab/bert-base-chinese'):
        print("ğŸ“¥ è¼‰å…¥ RoBERTa æ¨¡å‹...")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        
        # å®šç¾©ç”¢å“æ–¹é¢èˆ‡ç›¸é—œé—œéµè©
        self.aspects = {
            'é‡é‡é«”ç©': {
                'keywords': [
                    'è¼•', 'é‡', 'è–„', 'åš', 'å°', 'å¤§', 'é«”ç©', 'é‡é‡', 'å°ºå¯¸', 'ä¾¿æ”œ', 'æ”œå¸¶', 'è¼•è–„', 'è¼•å·§', 'è¢–ç',
                    'è¿·ä½ ', 'å¾®å‹', 'è¶…è–„', 'è¶…è¼•', 'è¶…å¤§', 'è¶…é‡', 'è¶…å°', 'è¶…åš', 'æ”œå¸¶æ–¹ä¾¿', 'æ”œå¸¶ä¸ä¾¿', 'ä¸ä½”ç©ºé–“', 'ä½”ç©ºé–“',
                    'é«”ç©å°', 'é«”ç©å¤§', 'é‡é‡è¼•', 'é‡é‡é‡', 'æ‰‹æ„Ÿé‡', 'æ‰‹æ„Ÿè¼•', 'æ˜“æ”œ', 'é›£æ”œ', 'æ–¹ä¾¿æ”œå¸¶', 'ä¸æ–¹ä¾¿æ”œå¸¶',
                    'å£è¢‹', 'åŒ…åŒ…', 'éš¨èº«', 'éš¨èº«æ”œå¸¶', 'æ”¶ç´', 'æ”¶ç´æ–¹ä¾¿', 'æ”¶ç´å›°é›£', 'é«”ç©é©ä¸­', 'é«”ç©é©åˆ', 'é«”ç©å‰›å¥½'
                ],
                'positive_terms': [
                    'è¼•', 'è–„', 'å°', 'ä¾¿æ”œ', 'è¼•å·§', 'è¢–ç', 'è¿·ä½ ', 'è¶…è–„', 'è¶…è¼•', 'ä¸ä½”ç©ºé–“', 'é«”ç©å°', 'é‡é‡è¼•',
                    'æ˜“æ”œ', 'æ–¹ä¾¿æ”œå¸¶', 'éš¨èº«', 'æ”¶ç´æ–¹ä¾¿', 'é«”ç©é©ä¸­', 'é«”ç©é©åˆ', 'é«”ç©å‰›å¥½'
                ],
                'negative_terms': [
                    'é‡', 'åš', 'å¤§', 'ç¬¨é‡', 'ä½”ç©ºé–“', 'è¶…å¤§', 'è¶…é‡', 'è¶…åš', 'æ”œå¸¶ä¸ä¾¿', 'é›£æ”œ', 'ä¸æ–¹ä¾¿æ”œå¸¶',
                    'é«”ç©å¤§', 'é‡é‡é‡', 'æ‰‹æ„Ÿé‡', 'æ”¶ç´å›°é›£'
                ]
            },
            'å……é›»é€Ÿåº¦': {
                'keywords': [
                    'å¿«å……', 'æ…¢', 'å¿«', 'é€Ÿåº¦', 'å……é›»', 'PD', 'QC', 'ç“¦æ•¸', 'W', 'å¿«é€Ÿ', 'é–ƒå……', 'æ€¥é€Ÿ', 'å……æ»¿',
                    'å……é›»å¿«', 'å……é›»æ…¢', 'å……é›»é€Ÿåº¦', 'å……é›»æ•ˆç‡', 'å……é›»æ™‚é–“', 'å……é›»å¾ˆå¿«', 'å……é›»å¾ˆæ…¢', 'å……é›»è¶…å¿«', 'å……é›»è¶…æ…¢',
                    'å……é›»è¿…é€Ÿ', 'å……é›»ç·©æ…¢', 'å……é›»ç­‰å¾…', 'å……é›»ç­‰å¾ˆä¹…', 'å……é›»ä¸ç”¨ç­‰', 'å……é›»é¦¬ä¸Š', 'å……é›»å³æ™‚', 'å……é›»å³åˆ»',
                    'å……é›»é«”é©—', 'å……é›»è¡¨ç¾', 'å……é›»éç¨‹', 'å……é›»æ™‚é•·', 'å……é›»æ™‚æ•ˆ', 'å……é›»åŠŸç‡', 'å……é›»æ”¯æ´', 'å¿«å……æ”¯æ´',
                    'å¿«å……åŠŸèƒ½', 'å¿«å……æ•ˆæœ', 'å¿«å……è¡¨ç¾', 'å¿«å……é«”é©—', 'å¿«å……å”è­°', 'å¿«å……æŠ€è¡“', 'å¿«å……æ¨™æº–', 'å¿«å……é€Ÿåº¦',
                    'é–ƒå……æŠ€è¡“', 'é–ƒå……é€Ÿåº¦', 'é–ƒå……æ•ˆæœ', 'é–ƒå……é«”é©—', 'é–ƒå……è¡¨ç¾', 'æ€¥é€Ÿå……é›»', 'æ¥µé€Ÿå……é›»', 'è¶…å¿«å……',
                    'è¶…æ…¢å……', 'æ…¢å……', 'æ…¢é€Ÿå……é›»', 'æ…¢é€Ÿ', 'å……é›»ç·©æ…¢', 'å……é›»æ‹–å»¶', 'å……é›»æ‹–å¾ˆä¹…', 'å……é›»ä¸ç©©', 'å……é›»ä¸é †',
                    'å……é›»å¡é “', 'å……é›»å¡ä½', 'å……é›»ä¸è‰¯', 'å……é›»ç•°å¸¸', 'å……é›»å•é¡Œ', 'å……é›»å›°é›£', 'å……é›»éšœç¤™'
                ],
                'positive_terms': [
                    'å¿«', 'å¿«é€Ÿ', 'æ€¥é€Ÿ', 'ç§’å……', 'é–ƒå……', 'å……é›»å¿«', 'å……é›»å¾ˆå¿«', 'å……é›»è¶…å¿«', 'å……é›»è¿…é€Ÿ', 'å……é›»ä¸ç”¨ç­‰',
                    'å……é›»é¦¬ä¸Š', 'å……é›»å³æ™‚', 'å……é›»å³åˆ»', 'å¿«å……', 'å¿«å……æ”¯æ´', 'å¿«å……åŠŸèƒ½', 'å¿«å……æ•ˆæœ', 'å¿«å……è¡¨ç¾',
                    'å¿«å……é«”é©—', 'å¿«å……å”è­°', 'å¿«å……æŠ€è¡“', 'å¿«å……æ¨™æº–', 'å¿«å……é€Ÿåº¦', 'é–ƒå……æŠ€è¡“', 'é–ƒå……é€Ÿåº¦', 'é–ƒå……æ•ˆæœ',
                    'é–ƒå……é«”é©—', 'é–ƒå……è¡¨ç¾', 'æ€¥é€Ÿå……é›»', 'æ¥µé€Ÿå……é›»', 'è¶…å¿«å……'
                ],
                'negative_terms': [
                    'æ…¢', 'é¾œé€Ÿ', 'ä¹…', 'ç­‰å¾ˆä¹…', 'å……é›»æ…¢', 'å……é›»å¾ˆæ…¢', 'å……é›»è¶…æ…¢', 'å……é›»ç·©æ…¢', 'å……é›»ç­‰å¾…',
                    'å……é›»ç­‰å¾ˆä¹…', 'æ…¢å……', 'æ…¢é€Ÿå……é›»', 'æ…¢é€Ÿ', 'å……é›»æ‹–å»¶', 'å……é›»æ‹–å¾ˆä¹…', 'å……é›»ä¸ç©©', 'å……é›»ä¸é †',
                    'å……é›»å¡é “', 'å……é›»å¡ä½', 'å……é›»ä¸è‰¯', 'å……é›»ç•°å¸¸', 'å……é›»å•é¡Œ', 'å……é›»å›°é›£', 'å……é›»éšœç¤™'
                ]
            },
            'æ¥å£ç›¸å®¹æ€§': {
                'keywords': [
                    'Type-C', 'Lightning', 'USB', 'æ¥å£', 'å­”', 'ç·š', 'ç›¸å®¹', 'é€šç”¨', 'è¬ç”¨', 'å¤šå£', 'Micro USB',
                    'USB-C', 'USB-A', 'USB3.0', 'USB2.0', 'PD', 'QC', 'å¿«å……å”è­°', 'å……é›»å”è­°', 'æ”¯æ´', 'ä¸æ”¯æ´',
                    'æ”¯æ´å¤šç¨®', 'æ”¯æ´å¤šå£', 'æ”¯æ´å¤šè¨­å‚™', 'æ”¯æ´å¤šå”è­°', 'å¤šè¨­å‚™', 'å¤šå”è­°', 'å¤šè£ç½®', 'å¤šå¹³å°', 'è·¨å¹³å°',
                    'è˜‹æœ', 'å®‰å“', 'iPhone', 'Android', 'iPad', 'Mac', 'Windows', 'ç­†é›»', 'æ‰‹æ©Ÿ', 'å¹³æ¿',
                    'è½‰æ¥é ­', 'è½‰æ¥ç·š', 'è½‰æ¥', 'è½‰æ›', 'è½‰æ›é ­', 'è½‰æ›ç·š', 'è½‰æ›å™¨', 'è½‰æ¥å™¨', 'æ’é ­', 'æ’å­”',
                    'æ’åº§', 'æ’æ§½', 'æ’å…¥', 'æ’æ‹”', 'æ’åˆ', 'æ’æ¥', 'æ’é…', 'æ’åˆæ€§', 'æ’é…æ€§', 'æ’æ¥æ€§',
                    'ç›¸å®¹æ€§', 'ä¸ç›¸å®¹', 'å…¼å®¹', 'ä¸å…¼å®¹', 'å…¼å®¹æ€§', 'ä¸å…¼å®¹æ€§', 'é©ç”¨', 'ä¸é©ç”¨', 'é©é…', 'ä¸é©é…',
                    'æ”¯æ´Type-C', 'æ”¯æ´Lightning', 'æ”¯æ´USB', 'æ”¯æ´Micro USB', 'æ”¯æ´USB-C', 'æ”¯æ´USB-A',
                    'æ”¯æ´PD', 'æ”¯æ´QC', 'æ”¯æ´å¿«å……', 'æ”¯æ´å……é›»å”è­°', 'æ”¯æ´å¤šç¨®å”è­°', 'æ”¯æ´å¤šç¨®è¨­å‚™', 'æ”¯æ´å¤šç¨®å¹³å°'
                ],
                'positive_terms': [
                    'é€šç”¨', 'ç›¸å®¹', 'è¬ç”¨', 'å¤šå£', 'é½Šå…¨', 'æ”¯æ´', 'æ”¯æ´å¤šç¨®', 'æ”¯æ´å¤šå£', 'æ”¯æ´å¤šè¨­å‚™', 'æ”¯æ´å¤šå”è­°',
                    'å¤šè¨­å‚™', 'å¤šå”è­°', 'å¤šè£ç½®', 'å¤šå¹³å°', 'è·¨å¹³å°', 'é©ç”¨', 'é©é…', 'å…¼å®¹', 'å…¼å®¹æ€§', 'æ”¯æ´Type-C',
                    'æ”¯æ´Lightning', 'æ”¯æ´USB', 'æ”¯æ´Micro USB', 'æ”¯æ´USB-C', 'æ”¯æ´USB-A', 'æ”¯æ´PD', 'æ”¯æ´QC',
                    'æ”¯æ´å¿«å……', 'æ”¯æ´å……é›»å”è­°', 'æ”¯æ´å¤šç¨®å”è­°', 'æ”¯æ´å¤šç¨®è¨­å‚™', 'æ”¯æ´å¤šç¨®å¹³å°', 'æ’é ­é½Šå…¨', 'æ’å­”é½Šå…¨'
                ],
                'negative_terms': [
                    'ä¸ç›¸å®¹', 'æ²’æœ‰', 'ç¼ºå°‘', 'åªæœ‰', 'ä¸æ”¯æ´', 'ä¸å…¼å®¹', 'ä¸å…¼å®¹æ€§', 'ä¸é©ç”¨', 'ä¸é©é…', 'ä¸æ”¯æ´Type-C',
                    'ä¸æ”¯æ´Lightning', 'ä¸æ”¯æ´USB', 'ä¸æ”¯æ´Micro USB', 'ä¸æ”¯æ´USB-C', 'ä¸æ”¯æ´USB-A', 'ä¸æ”¯æ´PD',
                    'ä¸æ”¯æ´QC', 'ä¸æ”¯æ´å¿«å……', 'ä¸æ”¯æ´å……é›»å”è­°', 'ä¸æ”¯æ´å¤šç¨®å”è­°', 'ä¸æ”¯æ´å¤šç¨®è¨­å‚™', 'ä¸æ”¯æ´å¤šç¨®å¹³å°',
                    'æ’é ­ç¼ºå°‘', 'æ’å­”ç¼ºå°‘', 'æ’é ­ä¸é½Š', 'æ’å­”ä¸é½Š', 'æ’é ­ä¸åˆ', 'æ’å­”ä¸åˆ', 'æ’é ­ä¸é…', 'æ’å­”ä¸é…'
                ]
            },
            'å¤–è§€æè³ª': {
                'keywords': [
                    'å¤–è§€', 'è³ªæ„Ÿ', 'æè³ª', 'è¨­è¨ˆ', 'é¡è‰²', 'ç¾', 'é†œ', 'å¥½çœ‹', 'å¡‘è† ', 'é‡‘å±¬', 'é‹åˆé‡‘', 'éœ§é¢',
                    'æ™‚å°š', 'æµè¡Œ', 'å¤–å‹', 'å¤–è¡¨', 'å¤–è§€è¨­è¨ˆ', 'å¤–è§€é€ å‹', 'å¤–è§€é¡è‰²', 'å¤–è§€è³ªæ„Ÿ', 'å¤–è§€æè³ª', 'å¤–è§€ç²¾ç·»',
                    'å¤–è§€é«˜ç´š', 'å¤–è§€æ¼‚äº®', 'å¤–è§€ç¾è§€', 'å¤–è§€æ™‚å°š', 'å¤–è§€æµè¡Œ', 'å¤–è§€å¤§æ–¹', 'å¤–è§€ç°¡ç´„', 'å¤–è§€ç¾ä»£',
                    'å¤–è§€æ–°ç©', 'å¤–è§€ç¨ç‰¹', 'å¤–è§€æœ‰å‹', 'å¤–è§€æœ‰è¨­è¨ˆæ„Ÿ', 'å¤–è§€æœ‰è³ªæ„Ÿ', 'å¤–è§€æœ‰ç‰¹è‰²', 'å¤–è§€æœ‰äº®é»',
                    'æ‰‹æ„Ÿ', 'æ‰‹æ„Ÿå¥½', 'æ‰‹æ„Ÿå·®', 'æ‰‹æ„Ÿèˆ’é©', 'æ‰‹æ„Ÿç²—ç³™', 'æ‰‹æ„Ÿç´°è†©', 'æ‰‹æ„Ÿæ»‘é †', 'æ‰‹æ„Ÿæ‰å¯¦',
                    'é¡è‰²æ¼‚äº®', 'é¡è‰²å¥½çœ‹', 'é¡è‰²é†œ', 'é¡è‰²å–®èª¿', 'é¡è‰²è±å¯Œ', 'é¡è‰²å¤šæ¨£', 'é¡è‰²æ™‚å°š', 'é¡è‰²æµè¡Œ',
                    'é¡è‰²æ–°ç©', 'é¡è‰²ç¨ç‰¹', 'é¡è‰²æœ‰è³ªæ„Ÿ', 'é¡è‰²æœ‰è¨­è¨ˆæ„Ÿ', 'é¡è‰²æœ‰ç‰¹è‰²', 'é¡è‰²æœ‰äº®é»',
                    'é‡‘å±¬æ„Ÿ', 'é‡‘å±¬è³ªæ„Ÿ', 'é‡‘å±¬å¤–è§€', 'é‡‘å±¬è¨­è¨ˆ', 'é‹åˆé‡‘å¤–è§€', 'é‹åˆé‡‘è¨­è¨ˆ', 'é‹åˆé‡‘è³ªæ„Ÿ',
                    'å¡‘è† æ„Ÿ', 'å¡‘è† å¤–è§€', 'å¡‘è† è¨­è¨ˆ', 'å¡‘è† è³ªæ„Ÿ', 'éœ§é¢å¤–è§€', 'éœ§é¢è¨­è¨ˆ', 'éœ§é¢è³ªæ„Ÿ',
                    'äº®é¢', 'äº®é¢å¤–è§€', 'äº®é¢è¨­è¨ˆ', 'äº®é¢è³ªæ„Ÿ', 'ç£¨ç ‚', 'ç£¨ç ‚å¤–è§€', 'ç£¨ç ‚è¨­è¨ˆ', 'ç£¨ç ‚è³ªæ„Ÿ'
                ],
                'positive_terms': [
                    'è³ªæ„Ÿ', 'é«˜ç´š', 'ç²¾ç·»', 'å¥½çœ‹', 'ç¾', 'æœ‰è³ªæ„Ÿ', 'æ™‚å°š', 'æµè¡Œ', 'æ¼‚äº®', 'å¤§æ–¹', 'ç°¡ç´„', 'ç¾ä»£',
                    'æ–°ç©', 'ç¨ç‰¹', 'æœ‰å‹', 'æœ‰è¨­è¨ˆæ„Ÿ', 'æœ‰ç‰¹è‰²', 'æœ‰äº®é»', 'æ‰‹æ„Ÿå¥½', 'æ‰‹æ„Ÿèˆ’é©', 'æ‰‹æ„Ÿç´°è†©',
                    'æ‰‹æ„Ÿæ»‘é †', 'æ‰‹æ„Ÿæ‰å¯¦', 'é¡è‰²æ¼‚äº®', 'é¡è‰²å¥½çœ‹', 'é¡è‰²è±å¯Œ', 'é¡è‰²å¤šæ¨£', 'é¡è‰²æ™‚å°š', 'é¡è‰²æµè¡Œ',
                    'é¡è‰²æ–°ç©', 'é¡è‰²ç¨ç‰¹', 'é¡è‰²æœ‰è³ªæ„Ÿ', 'é¡è‰²æœ‰è¨­è¨ˆæ„Ÿ', 'é¡è‰²æœ‰ç‰¹è‰²', 'é¡è‰²æœ‰äº®é»', 'é‡‘å±¬æ„Ÿ',
                    'é‡‘å±¬è³ªæ„Ÿ', 'é‡‘å±¬å¤–è§€', 'é‡‘å±¬è¨­è¨ˆ', 'é‹åˆé‡‘å¤–è§€', 'é‹åˆé‡‘è¨­è¨ˆ', 'é‹åˆé‡‘è³ªæ„Ÿ', 'éœ§é¢å¤–è§€',
                    'éœ§é¢è¨­è¨ˆ', 'éœ§é¢è³ªæ„Ÿ', 'äº®é¢å¤–è§€', 'äº®é¢è¨­è¨ˆ', 'äº®é¢è³ªæ„Ÿ', 'ç£¨ç ‚å¤–è§€', 'ç£¨ç ‚è¨­è¨ˆ', 'ç£¨ç ‚è³ªæ„Ÿ'
                ],
                'negative_terms': [
                    'å»‰åƒ¹', 'é†œ', 'å¡‘è† æ„Ÿ', 'ç²—ç³™', 'æ‰‹æ„Ÿå·®', 'é¡è‰²é†œ', 'é¡è‰²å–®èª¿', 'å¡‘è† å¤–è§€', 'å¡‘è† è¨­è¨ˆ',
                    'å¡‘è† è³ªæ„Ÿ', 'æ‰‹æ„Ÿç²—ç³™', 'æ‰‹æ„Ÿä¸ä½³', 'æ‰‹æ„Ÿæ»‘è†©', 'æ‰‹æ„Ÿé¬†æ•£', 'æ‰‹æ„Ÿä¸èˆ’æœ', 'æ‰‹æ„Ÿä¸é †',
                    'å¤–è§€å–®èª¿', 'å¤–è§€è€æ°£', 'å¤–è§€æ™®é€š', 'å¤–è§€ç„¡ç‰¹è‰²', 'å¤–è§€ç„¡äº®é»', 'å¤–è§€ä¸ç¾è§€', 'å¤–è§€ä¸æ™‚å°š',
                    'å¤–è§€ä¸æµè¡Œ', 'å¤–è§€ä¸å¤§æ–¹', 'å¤–è§€ä¸ç°¡ç´„', 'å¤–è§€ä¸ç¾ä»£', 'å¤–è§€ä¸æ–°ç©', 'å¤–è§€ä¸ç¨ç‰¹',
                    'å¤–è§€æ²’è¨­è¨ˆæ„Ÿ', 'å¤–è§€æ²’è³ªæ„Ÿ', 'å¤–è§€æ²’ç‰¹è‰²', 'å¤–è§€æ²’äº®é»', 'é‡‘å±¬æ„Ÿå·®', 'é‡‘å±¬è³ªæ„Ÿå·®',
                    'é‡‘å±¬å¤–è§€å·®', 'é‡‘å±¬è¨­è¨ˆå·®', 'é‹åˆé‡‘å¤–è§€å·®', 'é‹åˆé‡‘è¨­è¨ˆå·®', 'é‹åˆé‡‘è³ªæ„Ÿå·®', 'éœ§é¢å¤–è§€å·®',
                    'éœ§é¢è¨­è¨ˆå·®', 'éœ§é¢è³ªæ„Ÿå·®', 'äº®é¢å¤–è§€å·®', 'äº®é¢è¨­è¨ˆå·®', 'äº®é¢è³ªæ„Ÿå·®', 'ç£¨ç ‚å¤–è§€å·®',
                    'ç£¨ç ‚è¨­è¨ˆå·®', 'ç£¨ç ‚è³ªæ„Ÿå·®'
                ]
            },
            'åƒ¹æ ¼': {
                'keywords': [
                    'åƒ¹æ ¼', 'åƒ¹éŒ¢', 'åƒ¹å€¼', 'åƒ¹ä½', 'å”®åƒ¹', 'æ¨™åƒ¹', 'å®šåƒ¹', 'å¸‚åƒ¹', 'å…ƒ', 'å¡Š', 'é‡‘é¡',
                    'è²´', 'ä¾¿å®œ', 'åˆ’ç®—', 'è¶…å€¼', 'å¹³åƒ¹', 'å„ªæƒ ', 'æŠ˜æ‰£', 'ä¿ƒéŠ·', 'ç‰¹åƒ¹', 'æ€§åƒ¹æ¯”', 'CP', 'CPå€¼',
                    'å€¼å¾—', 'ä¸å€¼', 'æ˜‚è²´', 'ä¾¿å®œè²¨', 'é«˜åƒ¹', 'ä½åƒ¹', 'åƒ¹æ ¼åˆç†', 'åƒ¹æ ¼ä¸åˆç†', 'åƒ¹æ ¼åé«˜', 'åƒ¹æ ¼åä½',
                    'å‘éŒ¢', 'èŠ±è²»', 'èŠ±éŒ¢', 'èŠ±å¤ªå¤š', 'èŠ±å¾ˆå°‘', 'åˆ’ä¸ä¾†', 'åˆ’å¾—ä¾†', 'ç‰©è¶…æ‰€å€¼', 'ç‰©æœ‰æ‰€å€¼', 'åƒ¹æ ¼å¯¦æƒ ', 'åƒ¹æ ¼è¦ªæ°‘'
                ],
                'positive_terms': [
                    'ä¾¿å®œ', 'åˆ’ç®—', 'è¶…å€¼', 'å€¼å¾—', 'CPå€¼é«˜', 'å„ªæƒ ', 'æŠ˜æ‰£', 'ä¿ƒéŠ·', 'ç‰¹åƒ¹', 'æ€§åƒ¹æ¯”é«˜',
                    'åƒ¹æ ¼åˆç†', 'åƒ¹æ ¼è¦ªæ°‘', 'åƒ¹æ ¼å¯¦æƒ ', 'ç‰©è¶…æ‰€å€¼', 'ç‰©æœ‰æ‰€å€¼', 'å¹³åƒ¹', 'åˆ’å¾—ä¾†'
                ],
                'negative_terms': [
                    'è²´', 'æ˜‚è²´', 'ä¸å€¼', 'CPå€¼ä½', 'å‘éŒ¢', 'é«˜åƒ¹', 'åƒ¹æ ¼åé«˜', 'åƒ¹æ ¼ä¸åˆç†', 'èŠ±å¤ªå¤š', 'åˆ’ä¸ä¾†',
                    'åƒ¹æ ¼éé«˜', 'åƒ¹æ ¼å¤ªè²´', 'åƒ¹æ ¼å¤ªé«˜', 'åƒ¹æ ¼ä¸è¦ªæ°‘', 'åƒ¹æ ¼ä¸å¯¦æƒ ', 'åƒ¹æ ¼å¤ªé›¢è­œ', 'ä¸åˆ’ç®—'
                ]
            }
        }
        
        # å»ºç«‹æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆç°¡åŒ–ç‰ˆï¼Œå¯¦éš›æ‡‰è©²å¾®èª¿ï¼‰
        self.sentiment_classifier = pipeline(
            'sentiment-analysis',
            model='uer/roberta-base-finetuned-jd-binary-chinese',
            device=0 if torch.cuda.is_available() else -1
        )

    def _ensure_text(self, text) -> str:
        """ç¢ºä¿è¼¸å…¥æ˜¯å­—ä¸²ï¼›è‹¥ç‚º NaN/None å‰‡å›å‚³ç©ºå­—ä¸²ï¼Œå…¶ä»–é¡å‹è½‰ç‚º strã€‚"""
        try:
            # pandas çš„ NaN æ˜¯ float ä¸” pandas.isna å¯æª¢æ¸¬
            if pd.isna(text):
                return ''
        except Exception:
            # pd.isna å¯èƒ½åœ¨æŸäº›éæ¨™æº–ç‰©ä»¶ä¸Šä¸Ÿä¾‹å¤–ï¼Œä¿å®ˆè™•ç†
            pass

        if isinstance(text, str):
            return text
        return str(text)
    
    def extract_aspect_mentions(self, text: str) -> Dict[str, List[str]]:
        """æå–æ–‡æœ¬ä¸­æåŠçš„ç”¢å“æ–¹é¢"""
        mentions = {}
        # ç¢ºä¿ text ç‚ºå­—ä¸²ï¼Œé¿å… NaN/float ç­‰å‹åˆ¥é€ æˆ .split éŒ¯èª¤
        text = self._ensure_text(text)

        for aspect, info in self.aspects.items():
            # æ‰¾åˆ°åŒ…å«è©²æ–¹é¢é—œéµè©çš„å¥å­
            sentences = text.split('ã€‚')
            relevant_sentences = []
            
            for sentence in sentences:
                if any(keyword in sentence for keyword in info['keywords']):
                    relevant_sentences.append(sentence.strip())
            
            if relevant_sentences:
                mentions[aspect] = relevant_sentences
        
        return mentions
    
    def analyze_aspect_sentiment(self, text: str, aspect: str) -> Dict:
        """
        åˆ†æç‰¹å®šæ–¹é¢çš„æƒ…æ„Ÿ
        è¿”å›ï¼šæƒ…æ„Ÿæ¥µæ€§ã€åˆ†æ•¸ã€ç›¸é—œæ–‡æœ¬ç‰‡æ®µ
        """
        aspect_info = self.aspects.get(aspect)
        if not aspect_info:
            return None

        # ç¢ºä¿ text ç‚ºå­—ä¸²ï¼Œé¿å…å‚³å…¥ NaN/float æ™‚å‘¼å« split å¤±æ•—
        text = self._ensure_text(text)

        # æå–ç›¸é—œå¥å­
        sentences = text.split('ã€‚')
        relevant_sentences = [
            s for s in sentences 
            if any(keyword in s for keyword in aspect_info['keywords'])
        ]
        
        if not relevant_sentences:
            return {
                'mentioned': False,
                'sentiment': 'neutral',
                'score': 0.0,
                'confidence': 0.0,
                'evidence': []
            }
        
        # ä½¿ç”¨ RoBERTa é€²è¡Œæƒ…æ„Ÿåˆ†æ
        combined_text = 'ã€‚'.join(relevant_sentences)
        
        try:
            result = self.sentiment_classifier(combined_text[:512])[0]
            
            # è½‰æ›ç‚º -1 åˆ° 1 çš„åˆ†æ•¸
            if result['label'] == 'positive':
                score = result['score']
            else:
                score = -result['score']
            
            # é€²éšï¼šåŸºæ–¼æ­£é¢/è² é¢è©å½™é€²è¡Œå¾®èª¿
            pos_count = sum(1 for term in aspect_info['positive_terms'] if term in combined_text)
            neg_count = sum(1 for term in aspect_info['negative_terms'] if term in combined_text)
            
            # èª¿æ•´åˆ†æ•¸
            if pos_count > neg_count:
                score = max(score, 0.2)
            elif neg_count > pos_count:
                score = min(score, -0.2)
            
            # æé«˜æ­£è² é¢é–€æª»ï¼Œæ¸›å°‘åé —
            return {
                'mentioned': True,
                'sentiment': 'positive' if score > 0.2 else ('negative' if score < -0.4 else 'neutral'),
                'score': float(score),
                'confidence': float(result['score']),
                'evidence': relevant_sentences[:3],  # æœ€å¤šä¿ç•™3å€‹ä¾‹è­‰
                'pos_mentions': pos_count,
                'neg_mentions': neg_count
            }
        
        except Exception as e:
            print(f"æƒ…æ„Ÿåˆ†æéŒ¯èª¤: {e}")
            return {
                'mentioned': True,
                'sentiment': 'neutral',
                'score': 0.0,
                'confidence': 0.0,
                'evidence': relevant_sentences[:3]
            }
    
    def analyze_full_review(self, text: str) -> Dict:
        """åˆ†æå®Œæ•´è©•è«–çš„æ‰€æœ‰æ–¹é¢"""
        results = {}
        
        for aspect in self.aspects.keys():
            results[aspect] = self.analyze_aspect_sentiment(text, aspect)
        
        return results
    
    def batch_analyze(self, texts: List[str], show_progress=True) -> pd.DataFrame:
        """æ‰¹æ¬¡åˆ†æå¤šç¯‡è©•è«–"""
        from tqdm import tqdm
        
        all_results = []
        
        iterator = tqdm(texts) if show_progress else texts
        
        for text in iterator:
            # ç¢ºä¿æ–‡æœ¬ç‚ºå­—ä¸²å†åˆ†æ
            safe_text = self._ensure_text(text)
            analysis = self.analyze_full_review(safe_text)

            row = {'text': safe_text[:200]}  # ä¿ç•™å‰200å­—
            
            for aspect, result in analysis.items():
                row[f'{aspect}_mentioned'] = result['mentioned']
                row[f'{aspect}_score'] = result['score'] if result['mentioned'] else None
                row[f'{aspect}_sentiment'] = result['sentiment'] if result['mentioned'] else None
            
            all_results.append(row)
        
        return pd.DataFrame(all_results)


# =======================================
# 2. è©åµŒå…¥èˆ‡èªç¾©å¢å¼·
# =======================================

class SemanticEmbeddingAnalyzer:
    """
    åˆ©ç”¨ RoBERTa éš±è—å±¤å‘é‡ç”Ÿæˆè©åµŒå…¥
    å¯¦ç¾èªç¾©ç›¸ä¼¼åº¦ã€å¤šèªå€è©å½™å°æ‡‰ã€èªç¾©èšé¡
    """
    
    def __init__(self, model_name='ckiplab/bert-base-chinese'):
        print("ğŸ“¥ è¼‰å…¥è©åµŒå…¥æ¨¡å‹...")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        
        # è©å½™åº«
        self.vocabulary = {}
        self.embeddings_cache = {}
    
    def get_word_embedding(self, word: str) -> np.ndarray:
        """
        ç²å–å–®è©çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è©åµŒå…¥
        ä½¿ç”¨ RoBERTa éš±è—å±¤å‘é‡
        """
        if word in self.embeddings_cache:
            return self.embeddings_cache[word]
        
        # å°‡è©å½™æ”¾å…¥ç°¡å–®å¥å­ä¸­ä»¥ç²å¾—ä¸Šä¸‹æ–‡
        text = f"é€™å€‹ç”¢å“çš„{word}å¾ˆå¥½"
        
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨æœ€å¾Œä¸€å±¤çš„éš±è—ç‹€æ…‹
            hidden_states = outputs.last_hidden_state
            
            # æ‰¾åˆ°ç›®æ¨™è©çš„ä½ç½®ï¼ˆç°¡åŒ–è™•ç†ï¼‰
            word_embedding = hidden_states[0].mean(dim=0).cpu().numpy()
        
        self.embeddings_cache[word] = word_embedding
        return word_embedding
    
    def get_text_embedding(self, text: str) -> np.ndarray:
        """ç²å–æ•´æ®µæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨ [CLS] token çš„åµŒå…¥ä½œç‚ºæ–‡æœ¬è¡¨ç¤º
            cls_embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()
        
        return cls_embedding
    
    def calculate_semantic_similarity(self, word1: str, word2: str) -> float:
        """
        è¨ˆç®—å…©å€‹è©å½™çš„èªç¾©ç›¸ä¼¼åº¦
        æ‡‰ç”¨ä¸€ï¼šå¤šèªå€è©å½™å°æ‡‰
        """
        emb1 = self.get_word_embedding(word1)
        emb2 = self.get_word_embedding(word2)
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity([emb1], [emb2])[0][0]
        
        return float(similarity)
    
    def find_similar_terms(self, target_word: str, candidate_words: List[str], 
                          top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ‰¾åˆ°èˆ‡ç›®æ¨™è©èªç¾©ç›¸ä¼¼çš„è©å½™
        å¯ç”¨æ–¼ç™¼ç¾åŒç¾©è©ã€åœ°å€ç”¨èªå·®ç•°
        """
        target_emb = self.get_word_embedding(target_word)
        
        similarities = []
        for word in candidate_words:
            if word == target_word:
                continue
            word_emb = self.get_word_embedding(word)
            sim = cosine_similarity([target_emb], [word_emb])[0][0]
            similarities.append((word, float(sim)))
        
        # æ’åºä¸¦è¿”å› top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def semantic_clustering(self, words: List[str], n_clusters: int = 5) -> Dict:
        """
        èªç¾©èšé¡ï¼šå°‡è©å½™æŒ‰èªç¾©è‡ªå‹•åˆ†çµ„
        ä¾‹å¦‚ï¼šã€Œè¡Œå‹•é›»æºã€ã€ã€Œå……é›»å¯¶ã€ã€ã€Œå°¿è¢‹ã€æœƒèšæˆä¸€é¡
        """
        print(f"ğŸ”„ å° {len(words)} å€‹è©å½™é€²è¡Œèªç¾©èšé¡...")
        
        # ç²å–æ‰€æœ‰è©å½™çš„åµŒå…¥
        embeddings = []
        valid_words = []
        
        for word in words:
            try:
                emb = self.get_word_embedding(word)
                embeddings.append(emb)
                valid_words.append(word)
            except:
                continue
        
        embeddings = np.array(embeddings)
        
        # K-means èšé¡
        kmeans = KMeans(n_clusters=min(n_clusters, len(valid_words)), random_state=42)
        clusters = kmeans.fit_predict(embeddings)
        
        # çµ„ç¹”çµæœ
        cluster_dict = {}
        for i in range(n_clusters):
            cluster_words = [valid_words[j] for j in range(len(valid_words)) if clusters[j] == i]
            if cluster_words:
                cluster_dict[f'ç¾¤é›†_{i+1}'] = cluster_words
        
        return cluster_dict
    
    def visualize_semantic_space(self, texts: List[str], labels: List[str] = None,
                                 method: str = 'umap') -> go.Figure:
        """
        æ‡‰ç”¨äºŒï¼šèªç¾©å¯è¦–åŒ–
        ä½¿ç”¨ UMAP æˆ– t-SNE é™ç¶­ä¸¦è¦–è¦ºåŒ–è©å‘é‡ç©ºé–“
        """
        print(f"ğŸ¨ ä½¿ç”¨ {method.upper()} é€²è¡Œèªç¾©ç©ºé–“è¦–è¦ºåŒ–...")
        
        # ç²å–æ–‡æœ¬åµŒå…¥
        embeddings = []
        for text in texts:
            emb = self.get_text_embedding(text)
            embeddings.append(emb)
        
        embeddings = np.array(embeddings)
        
        # é™ç¶­
        if method == 'umap':
            reducer = umap.UMAP(n_components=2, random_state=42)
            embeddings_2d = reducer.fit_transform(embeddings)
        else:  # t-SNE
            from sklearn.manifold import TSNE
            reducer = TSNE(n_components=2, random_state=42)
            embeddings_2d = reducer.fit_transform(embeddings)
        
        # å‰µå»ºè¦–è¦ºåŒ–
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'text': [t[:50] + '...' for t in texts],
            'label': labels if labels else ['æœªåˆ†é¡'] * len(texts)
        })
        
        fig = px.scatter(
            df, x='x', y='y', color='label',
            hover_data=['text'],
            title=f'èªç¾©ç©ºé–“è¦–è¦ºåŒ– ({method.upper()})',
            width=800, height=600
        )
        
        fig.update_layout(
            xaxis_title=f'{method.upper()} ç¶­åº¦ 1',
            yaxis_title=f'{method.upper()} ç¶­åº¦ 2'
        )
        
        return fig


# =======================================
# 3. é›¶æ¨£æœ¬å­¸ç¿’æŠ€è¡“åµæ¸¬
# =======================================

class ZeroShotTechDetector:
    """
    ä½¿ç”¨é›¶æ¨£æœ¬å­¸ç¿’å³æ™‚è­˜åˆ¥æ–°èˆˆæŠ€è¡“è©
    ç„¡éœ€é å…ˆæ¨™è¨»ï¼Œè‡ªå‹•åµæ¸¬ GaNã€Qi2 ç­‰æŠ€è¡“é—œéµè©
    """
    
    def __init__(self):
        print("ğŸ“¥ è¼‰å…¥é›¶æ¨£æœ¬å­¸ç¿’æ¨¡å‹...")
        self.classifier = pipeline(
            "zero-shot-classification",
            model="MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7",
            device=0 if torch.cuda.is_available() else -1
        )
        
        # å®šç¾©æŠ€è¡“é¡åˆ¥ï¼ˆå¯å‹•æ…‹æ“´å±•ï¼‰
        self.tech_categories = {
            'å……é›»æŠ€è¡“': ['å¿«å……', 'PD', 'QC', 'é–ƒå……', 'ç„¡ç·šå……é›»', 'Qi', 'Qi2'],
            'ææ–™æŠ€è¡“': ['GaN', 'æ°®åŒ–éµ', 'çŸ½ææ–™', 'çŸ³å¢¨çƒ¯'],
            'é›»æ± æŠ€è¡“': ['é‹°é›»æ± ', 'å›ºæ…‹é›»æ± ', 'ç£·é…¸éµé‹°', 'ä¸‰å…ƒé‹°'],
            'æ¥å£æ¨™æº–': ['Type-C', 'Lightning', 'USB-A', 'Micro USB'],
            'å®‰å…¨æŠ€è¡“': ['éå……ä¿è­·', 'æº«æ§', 'çŸ­è·¯ä¿è­·', 'BMS']
        }
    
    def detect_technology(self, text: str, threshold: float = 0.5) -> List[Dict]:
        """
        é›¶æ¨£æœ¬åµæ¸¬æ–‡æœ¬ä¸­çš„æŠ€è¡“é¡åˆ¥
        """
        categories = list(self.tech_categories.keys())
        
        result = self.classifier(
            text,
            candidate_labels=categories,
            multi_label=True
        )
        
        detected_techs = []
        for label, score in zip(result['labels'], result['scores']):
            if score > threshold:
                detected_techs.append({
                    'category': label,
                    'confidence': float(score),
                    'keywords': self.tech_categories[label]
                })
        
        return detected_techs
    
    def extract_emerging_keywords(self, texts: List[str], min_frequency: int = 3) -> Dict:
        """
        å¾å¤§é‡æ–‡æœ¬ä¸­æå–æ–°èˆˆæŠ€è¡“é—œéµè©
        """
        # åˆ†è©ä¸¦çµ±è¨ˆè©é »
        all_words = []
        for text in texts:
            words = list(jieba.cut(text))
            all_words.extend(words)
        
        from collections import Counter
        word_freq = Counter(all_words)
        
        # ç¯©é¸å¯èƒ½çš„æŠ€è¡“è©ï¼ˆ2-4å€‹å­—ï¼Œå‡ºç¾é »ç‡é©ä¸­ï¼‰
        tech_candidates = [
            word for word, freq in word_freq.items()
            if 2 <= len(word) <= 4 and min_frequency <= freq <= len(texts) * 0.3
        ]
        
        # ä½¿ç”¨é›¶æ¨£æœ¬åˆ†é¡é©—è­‰
        emerging_techs = {}
        
        for word in tech_candidates[:50]:  # é™åˆ¶æ•¸é‡é¿å…å¤ªæ…¢
            sample_text = f"é€™å€‹ç”¢å“ä½¿ç”¨äº†{word}æŠ€è¡“"
            detection = self.detect_technology(sample_text, threshold=0.4)
            
            if detection:
                category = detection[0]['category']
                if category not in emerging_techs:
                    emerging_techs[category] = []
                emerging_techs[category].append({
                    'keyword': word,
                    'frequency': word_freq[word],
                    'confidence': detection[0]['confidence']
                })
        
        return emerging_techs
    
    def track_tech_trends(self, df: pd.DataFrame, date_col: str = 'date',
                         text_col: str = 'text') -> pd.DataFrame:
        """
        è¿½è¹¤æŠ€è¡“è©çš„æ™‚é–“è¶¨å‹¢
        è¿”å›å„æŠ€è¡“é¡åˆ¥éš¨æ™‚é–“çš„è¨è«–ç†±åº¦
        """
        df[date_col] = pd.to_datetime(df[date_col])
        df['month'] = df[date_col].dt.to_period('M')
        
        trends = []
        
        for month in df['month'].unique():
            month_data = df[df['month'] == month]
            month_texts = ' '.join(month_data[text_col].tolist())
            
            # åµæ¸¬è©²æœˆçš„æŠ€è¡“åˆ†ä½ˆ
            for category, keywords in self.tech_categories.items():
                mention_count = sum(
                    text.count(keyword) 
                    for text in month_data[text_col] 
                    for keyword in keywords
                )
                
                trends.append({
                    'month': str(month),
                    'category': category,
                    'mentions': mention_count,
                    'docs': len(month_data)
                })
        
        trends_df = pd.DataFrame(trends)
        trends_df['mention_rate'] = trends_df['mentions'] / trends_df['docs']
        
        return trends_df


# =======================================
# 4. æ•´åˆåˆ†æå¼•æ“
# =======================================

class AdvancedNLPEngine:
    """
    PowerPulse é€²éš NLP å¼•æ“æ•´åˆ
    çµåˆ ABSAã€è©åµŒå…¥ã€é›¶æ¨£æœ¬å­¸ç¿’
    """
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ– PowerPulse é€²éš NLP å¼•æ“...")
        self.absa = AspectBasedSentimentAnalyzer()
        self.semantic = SemanticEmbeddingAnalyzer()
        self.tech_detector = ZeroShotTechDetector()
        print("âœ… NLP å¼•æ“å°±ç·’ï¼")
    
    def full_analysis(self, texts: List[str], dates: List[str] = None) -> Dict:
        """
        åŸ·è¡Œå®Œæ•´çš„ NLP åˆ†æç®¡ç·š
        """
        print("\n" + "="*50)
        print("é–‹å§‹å®Œæ•´ NLP åˆ†æ...")
        print("="*50 + "\n")
        
        results = {}
        
        # 1. æ–¹é¢ç´šæƒ…æ„Ÿåˆ†æ
        print("ğŸ“Š åŸ·è¡Œæ–¹é¢ç´šæƒ…æ„Ÿåˆ†æ...")
        absa_results = self.absa.batch_analyze(texts)
        results['sentiment_analysis'] = absa_results
        
        # 2. èªç¾©èšé¡åˆ†æ
        print("\nğŸ”„ åŸ·è¡Œèªç¾©èšé¡...")
        # æå–é«˜é »è©é€²è¡Œèšé¡
        all_words = []
        for text in texts:
            words = list(jieba.cut(text))
            all_words.extend([w for w in words if len(w) >= 2])
        
        from collections import Counter
        top_words = [word for word, _ in Counter(all_words).most_common(50)]
        
        clusters = self.semantic.semantic_clustering(top_words, n_clusters=5)
        results['semantic_clusters'] = clusters
        
        # 3. èªç¾©ç©ºé–“è¦–è¦ºåŒ–
        print("\nğŸ¨ ç”Ÿæˆèªç¾©ç©ºé–“è¦–è¦ºåŒ–...")
        semantic_viz = self.semantic.visualize_semantic_space(
            texts[:100],  # é™åˆ¶æ•¸é‡
            method='umap'
        )
        results['semantic_visualization'] = semantic_viz
        
        # 4. é›¶æ¨£æœ¬æŠ€è¡“åµæ¸¬
        print("\nğŸ” åŸ·è¡Œé›¶æ¨£æœ¬æŠ€è¡“åµæ¸¬...")
        emerging_techs = self.tech_detector.extract_emerging_keywords(texts)
        results['emerging_technologies'] = emerging_techs
        
        # 5. æŠ€è¡“è¶¨å‹¢è¿½è¹¤ï¼ˆå¦‚æœæœ‰æ—¥æœŸï¼‰
        if dates:
            print("\nğŸ“ˆ è¿½è¹¤æŠ€è¡“è¶¨å‹¢...")
            df = pd.DataFrame({'text': texts, 'date': dates})
            tech_trends = self.tech_detector.track_tech_trends(df)
            results['tech_trends'] = tech_trends
        
        print("\nâœ… å®Œæ•´ NLP åˆ†æå®Œæˆï¼")
        return results
    
    def generate_insights_report(self, results: Dict) -> str:
        """ç”Ÿæˆæ´å¯Ÿå ±å‘Š"""
        report = []
        report.append("="*60)
        report.append("PowerPulse AI - é€²éš NLP åˆ†æå ±å‘Š")
        report.append("="*60)
        
        # æƒ…æ„Ÿåˆ†ææ‘˜è¦
        sentiment_df = results['sentiment_analysis']
        report.append("\nã€æ–¹é¢ç´šæƒ…æ„Ÿåˆ†æã€‘")
        
        aspects = ['é‡é‡é«”ç©', 'å……é›»é€Ÿåº¦', 'æ¥å£ç›¸å®¹æ€§', 'å¤–è§€æè³ª', 'åƒ¹æ ¼']
        for aspect in aspects:
            mentioned = sentiment_df[f'{aspect}_mentioned'].sum()
            if mentioned > 0:
                avg_score = sentiment_df[sentiment_df[f'{aspect}_mentioned']][f'{aspect}_score'].mean()
                report.append(f"  â€¢ {aspect}: æåŠ {mentioned} æ¬¡, å¹³å‡æƒ…æ„Ÿ {avg_score:.2f}")
        
        # èªç¾©èšé¡æ‘˜è¦
        report.append("\nã€èªç¾©èšé¡ç™¼ç¾ã€‘")
        clusters = results['semantic_clusters']
        for cluster_name, words in list(clusters.items())[:3]:
            report.append(f"  â€¢ {cluster_name}: {', '.join(words[:5])}")
        
        # æ–°èˆˆæŠ€è¡“
        report.append("\nã€æ–°èˆˆæŠ€è¡“åµæ¸¬ã€‘")
        emerging = results['emerging_technologies']
        for category, tech_list in emerging.items():
            top_tech = tech_list[0] if tech_list else None
            if top_tech:
                report.append(f"  â€¢ {category}: {top_tech['keyword']} (æåŠ {top_tech['frequency']} æ¬¡)")
        
        report.append("\n" + "="*60)
        
        return '\n'.join(report)


# =======================================
# ä½¿ç”¨ç¯„ä¾‹
# =======================================

if __name__ == "__main__":
    # åˆå§‹åŒ–å¼•æ“
    nlp_engine = AdvancedNLPEngine()

    # ä¿®æ­£è³‡æ–™å¤¾åç¨±: crawelers_result -> crawlers_result
    data = pd.read_csv('./AICompetition/crawlers_result/data_mobile.csv')
    data1=list(data['title'])
    data2=list(data['comments'])
    data3=data1+data2
    data3=[x for x in data3 if str(x).lower() not in ('nan', 'none')]
    # æ¸¬è©¦æ•¸æ“š
    sample_texts = [
        "é€™å€‹è¡Œå‹•é›»æºè¶…è¼•è–„ï¼Œå……é›»é€Ÿåº¦å¾ˆå¿«ï¼Œæ”¯æ´PDå¿«å……ï¼Œè³ªæ„Ÿä¹Ÿä¸éŒ¯",
        "å……é›»å¯¶å¤ªé‡äº†ï¼Œè€Œä¸”å……é›»å¾ˆæ…¢ï¼Œä¸æ”¯æ´Type-Cå¾ˆä¸æ–¹ä¾¿",
        "GaNæŠ€è¡“çœŸçš„ä¸éŒ¯ï¼Œå……é›»è¶…å¿«ï¼Œå°±æ˜¯åƒ¹æ ¼æœ‰é»è²´",
        "é€™æ¬¾å°¿è¢‹å¾ˆè¼•ä¾¿ï¼Œä½†å®¹é‡å¤ªå°äº†ï¼Œå¤–è§€è¨­è¨ˆå¾ˆç¾",
        "æ”¯æ´Qi2ç„¡ç·šå……é›»å¾ˆæ–¹ä¾¿ï¼Œä¸éåƒ¹æ ¼åé«˜"
    ]
    
    dates = ['2025-11-15'] * len(data3)
    
    # åŸ·è¡Œå®Œæ•´åˆ†æ
    results = nlp_engine.full_analysis(data3, dates)
    
    # ç”Ÿæˆå ±å‘Š
    report = nlp_engine.generate_insights_report(results)
    print(report)
    
    # ä¿å­˜çµæœ
    results['sentiment_analysis'].to_csv('absa_results.csv', index=False, encoding='utf-8-sig')
    print("\nğŸ’¾ çµæœå·²ä¿å­˜è‡³ absa_results.csv")